{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409a5439",
   "metadata": {},
   "source": [
    "BASICS OF NLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ffc4f",
   "metadata": {},
   "source": [
    "NEED TO UNDERSTAND FIRST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcb970",
   "metadata": {},
   "source": [
    "sklearn(manifold.\"TSNE\") AND sklearn(decomposition.\"PCA\") CAN BE USED FOR DIMENSIONALITY REDUCTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85ce101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kousimon\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Kousimon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy \n",
    "import re \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "import numpy as np\n",
    "import transformers\n",
    "import torchtext\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c1f3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text =  \"Koushick is %#@$%##% learning Natural!@$@#$@# Language Processing from ChatGPT.\"\n",
    "Cleaned_words = re.sub(r'[^a-zA-Z0-9\\s]',\" \",text)\n",
    "cleaned_txt = Cleaned_words.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f885a3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "koushick\n",
      "         \n",
      "learning\n",
      "natural\n",
      "        \n",
      "language\n",
      "processing\n",
      "chatgpt\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(cleaned_txt)\n",
    "\n",
    "filtered_tokens = []\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for token in doc:\n",
    "    if token.text not in STOP_WORDS and not token.is_punct:\n",
    "        print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657e657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of Speech Tags\n",
      "koushick -> PROPN\n",
      "is -> AUX\n",
      "          -> SPACE\n",
      "learning -> VERB\n",
      "natural -> ADJ\n",
      "         -> SPACE\n",
      "language -> NOUN\n",
      "processing -> NOUN\n",
      "from -> ADP\n",
      "chatgpt -> NOUN\n"
     ]
    }
   ],
   "source": [
    "print(\"Part of Speech Tags\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f'{token.text} -> {token.pos_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a2112b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " ['and' 'chatgpt' 'exciting' 'fun' 'is' 'learning' 'love' 'makes' 'more'\n",
      " 'nlp']\n",
      "\n",
      "Bag of Words Matrix:\n",
      "[[1 0 1 1 1 0 0 0 0 1]\n",
      " [0 1 0 1 0 0 0 1 1 1]\n",
      " [0 0 0 0 0 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "## Vectorizing\n",
    "## BAG OF WORDS \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "docs = [\n",
    "    \"NLP is fun and exciting\",\n",
    "    \"ChatGPT makes NLP more fun\",\n",
    "    \"I love learning NLP\"\n",
    "]\n",
    "\n",
    "# Create vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Get vocabulary\n",
    "print(\"Vocabulary:\\n\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Show BoW Matrix\n",
    "print(\"\\nBag of Words Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a81b3009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      " ['and' 'chatgpt' 'exciting' 'fun' 'is' 'learning' 'love' 'makes' 'more'\n",
      " 'nlp']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.50461134 0.         0.50461134 0.38376993 0.50461134 0.\n",
      "  0.         0.         0.         0.29803159]\n",
      " [0.         0.50461134 0.         0.38376993 0.         0.\n",
      "  0.         0.50461134 0.50461134 0.29803159]\n",
      " [0.         0.         0.         0.         0.         0.65249088\n",
      "  0.65249088 0.         0.         0.38537163]]\n"
     ]
    }
   ],
   "source": [
    "## Vectorizing\n",
    "## TERM FREQUENCY INVERSE DOCUMENT FREQUENCY (TF-IDF)\n",
    "docs = [\n",
    "    \"NLP is fun and exciting\",\n",
    "    \"ChatGPT makes NLP more fun\",\n",
    "    \"I love learning NLP\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "X = tfidf.fit_transform(docs)\n",
    "\n",
    "# Get vocabulary\n",
    "print(\"Vocabulary:\\n\", tfidf.get_feature_names_out())\n",
    "\n",
    "# Show TF-IDF Matrix\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830a4f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nlp', 'is', 'fun', 'and', 'powerful', 'than', 'other', 'field', 'of', 'nlp']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda2764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'field', 'fun', 'is', 'nlp', 'of', 'other', 'powerful', 'than']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b09274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'field': 1,\n",
       " 'fun': 2,\n",
       " 'is': 3,\n",
       " 'nlp': 4,\n",
       " 'of': 5,\n",
       " 'other': 6,\n",
       " 'powerful': 7,\n",
       " 'than': 8}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6079999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp        -> [0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "is         -> [0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "fun        -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "and        -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "powerful   -> [0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "than       -> [0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "other      -> [0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "field      -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "of         -> [0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "nlp        -> [0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encoding(word,vocab):\n",
    "    vector = [0]*len(vocab)\n",
    "    idx = word2idx[word]\n",
    "    vector[idx] = 1\n",
    "    return vector\n",
    "\n",
    "for word in tokens:\n",
    "    print(f'{word:10} -> {one_hot_encoding(word,vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f21a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking whether the cleaned text is empty or not  (IF NEEDED PURPOSE ONLY)\n",
    "clean = re.sub(r'[^a-zA-Z\\s]', \"\", \"@2025!\")\n",
    "print(repr(clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edde132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i', 'love', 'nlp'], ['nlp', 'is', 'powerful'], ['welcome', 'to', 'nlp']]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I love NLP!\",\n",
    "    \"NLP is powerful 100%.\",\n",
    "    \"Welcome to NLP @2025!\"\n",
    "]\n",
    "\n",
    "tokenized = [re.sub(r'[^a-zA-Z\\s]', \"\", sent).lower().split() for sent in sentences]\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb55028",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(word for sent in tokenized for word in sent))\n",
    "Word2idx = {word:idx for word,idx in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a23e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'is', 'love', 'nlp', 'powerful', 'to', 'welcome']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282f973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'i', 1: 'is', 2: 'love', 3: 'nlp', 4: 'powerful', 5: 'to', 6: 'welcome'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ebc61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100%.',\n",
       " '@2025!',\n",
       " 'i',\n",
       " 'is',\n",
       " 'love',\n",
       " 'nlp',\n",
       " 'nlp!',\n",
       " 'powerful',\n",
       " 'to',\n",
       " 'welcome']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b705e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '100%.',\n",
       " 1: '@2025!',\n",
       " 2: 'i',\n",
       " 3: 'is',\n",
       " 4: 'love',\n",
       " 5: 'nlp',\n",
       " 6: 'nlp!',\n",
       " 7: 'powerful',\n",
       " 8: 'to',\n",
       " 9: 'welcome'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a02bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"NLP is a branch of artificial intelligence.\",\n",
    "    \"Deep learning models perform well on text data.\",\n",
    "    \"Transformers have revolutionized NLP.\",\n",
    "    \"Word embeddings capture semantic meaning.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57747f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd4e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv(\"nlp_sentences_50.csv\")\n",
    "sentences = df[\"text\"].astype(str).tolist()\n",
    "\n",
    "# Tokenize\n",
    "tokenized = [word_tokenize(s.lower()) for s in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Flatten and encode\n",
    "vocab = sorted(set(word for sent in tokenized for word in sent))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "# One-hot encode all words\n",
    "def one_hot_encode(tokens, vocab_size, word2idx):\n",
    "    return [np.eye(vocab_size)[word2idx[word]] for word in tokens if word in word2idx]\n",
    "\n",
    "one_hot_sentences = [one_hot_encode(sent, vocab_size, word2idx) for sent in tokenized]\n",
    "\n",
    "print(\"Sample one-hot vector (word-level):\", one_hot_sentences[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233de396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Recombine tokens for BoW\n",
    "rejoined = [' '.join(tokens) for tokens in tokenized]\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(rejoined).toarray()\n",
    "\n",
    "print(\"BoW shape:\", bow_matrix.shape)\n",
    "print(\"Sample BoW vector:\", bow_matrix[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af9b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(rejoined).toarray()\n",
    "\n",
    "print(\"TF-IDF shape:\", tfidf_matrix.shape)\n",
    "print(\"Sample TF-IDF vector:\", tfidf_matrix[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2032c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 50\n",
    "word_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Convert sentences to word indices\n",
    "indexed = [[word2idx[word] for word in sent if word in word2idx] for sent in tokenized]\n",
    "\n",
    "# Embed each sentence\n",
    "sentence_vectors = []\n",
    "for idx_list in indexed:\n",
    "    idx_tensor = torch.tensor(idx_list)\n",
    "    embeddings = word_embedding(idx_tensor)\n",
    "    sentence_embedding = embeddings.mean(dim=0)\n",
    "    sentence_vectors.append(sentence_embedding)\n",
    "\n",
    "sentence_vectors = torch.stack(sentence_vectors)\n",
    "print(\"Random embedding vector shape:\", sentence_vectors.shape)\n",
    "print(\"Sample embedding vector:\", sentence_vectors[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6812169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# Load GloVe 100D\n",
    "glove = GloVe(name='6B', dim=100)\n",
    "\n",
    "# For each tokenized sentence, convert each word to glove vector, then average\n",
    "glove_sentences = []\n",
    "for tokens in tokenized:\n",
    "    vectors = []\n",
    "    for word in tokens:\n",
    "        if word in glove.stoi:\n",
    "            vectors.append(glove[word])\n",
    "        else:\n",
    "            vectors.append(torch.zeros(100))  # Unknown words\n",
    "    if vectors:\n",
    "        avg_vec = torch.stack(vectors).mean(dim=0)\n",
    "    else:\n",
    "        avg_vec = torch.zeros(100)\n",
    "    glove_sentences.append(avg_vec)\n",
    "\n",
    "glove_sentences = torch.stack(glove_sentences)\n",
    "\n",
    "print(\"GloVe sentence vector shape:\", glove_sentences.shape)\n",
    "print(\"Sample GloVe sentence vector:\", glove_sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Get BERT sentence embeddings\n",
    "bert_sentence_embeddings = []\n",
    "\n",
    "for sentence in tqdm(sentences, desc=\"Generating BERT embeddings\"):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding='max_length', max_length=50)\n",
    "        outputs = bert_model(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[0, 0, :]  # [CLS] token representation\n",
    "        bert_sentence_embeddings.append(cls_embedding)\n",
    "\n",
    "bert_sentence_embeddings = torch.stack(bert_sentence_embeddings)\n",
    "print(\"BERT embeddings shape:\", bert_sentence_embeddings.shape)\n",
    "print(\"Sample BERT vector:\", bert_sentence_embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ace6082",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
